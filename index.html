<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Tactile foundation model enabling zero-shot transfer across optical tactile sensors by extracting sensor-invariant tactile representations.">
  <meta property="og:title" content="Sensor-Invariant Tactile Representation"/>
  <meta property="og:description" content="Tactile foundation model enabling zero-shot transfer across optical tactile sensors by extracting sensor-invariant tactile representations."/>
  <meta property="og:url" content="https://hgupt3.github.io/sitr/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/arch.png" />
  <meta property="og:image:width" content="1496"/>
  <meta property="og:image:height" content="805"/>


  <meta name="twitter:title" content="Sensor-Invariant Tactile Representation">
  <meta name="twitter:description" content="Tactile foundation model enabling zero-shot transfer across optical tactile sensors by extracting sensor-invariant tactile representations.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/arch.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Sensor-Invariant Tactile Representation</title>
  <link rel="icon" href="https://cdn.brand.illinois.edu/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
          delimiters: [
              {left: "$$", right: "$$", display: true},  // Block math
              {left: "$", right: "$", display: false}    // Inline math
          ]
      });
  });
</script>


<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Sensor-Invariant Tactile Representation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://hgupt3.github.io/" target="_blank">Harsh Gupta</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=zzpPyQwAAAAJ&hl=en" target="_blank">Yuchen Mo</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://leumasnij.github.io/" target="_blank">Shengmiao Jin</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=SNqm6doAAAAJ&hl=en" target="_blank">Wenzhen Yuan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Illinois Urbana-Champaign<br>International Conference on Learning Representations (ICLR), 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>equal contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2502.19638" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                        </a>
                      </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->


                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.19638" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span>
                
                <!-- Github link -->
                <span class="link-block">
                  <!-- <a href="https://github.com/hgupt3/gsrl" target="_blank" -->
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code [coming soon]</span>
                  </a>
                </span>

                <span class="link-block">
                  <!-- <a href="https://huggingface.co/datasets/hgupt3/sitr_dataset" target="_blank" -->
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset + Weights [coming soon]</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<div class="container is-max-desktop" style="margin-top: -50px;">
  <div class="hero-body" style="padding-top: 0;">
    <div class="item">
      <!-- Your image here -->
      <video width=100% muted autoplay loop>
        <source src="static/images/sitr.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    <h2 class="subtitle has-text-centered">
      TL;DR Sensor-Invariant Tactile Representation (SITR)
    </h2>
  </div>
</div>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            High-resolution tactile sensors have become critical for embodied perception and robotic manipulation. However, a key challenge in the field is the lack of transferability between sensors due to design and manufacturing variations, which result in significant differences in tactile signals. This limitation hinders the ability to transfer models or knowledge learned from one sensor to another. To address this, we introduce a novel method for extracting Sensor-Invariant Tactile Representations (SITR), enabling zero-shot transfer across optical tactile sensors. Our approach utilizes a transformer-based architecture trained on a diverse dataset of simulated sensor designs, allowing it to generalize to new sensors in the real world with minimal calibration. Experimental results demonstrate the method's effectiveness across various tactile sensing applications, facilitating data and model transferability for future advancements in the field.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <img src="static/images/arch.png" alt="architecture" class="image is-fullwidth"/>
        <div class="content has-text-justified">
          <br><br>
          <p>
            Our sensor-invariant representation learning framework. Each tactile image $x$ is paired with a set of calibration images $c$. We patchify and linearly project $x$ and $c$ to tokens. Additionally, the $c$ patches are region-wise stacked before projection. We concatenate the input tokens with a class token $z$ and pass it through a transformer encoder. The class token $z$ is trained with supervised contrastive learning, while patch tokens are supervised by normal map reconstruction loss. We highlight in grey the concatenation of the output class token and patch tokens as our Sensor-Invariant Tactile Representation (SITR) for downstream tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<!-- End image carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 is-size-4">Calibration Tokens</h2>
          <div class="is-flex is-justify-content-center">
            <img src="static/images/calibration.png" alt="blender" class="image" style="max-width:60%;"/>
          </div>
          <div class="content has-text-justified">
            <br><br>
            <p>
              GelSight-like sensors map RGB values at each pixel to the local surface gradient, enabling the reconstruction of the contact surface. However, these sensors exhibit variations in physical properties that introduce sensor-specific artifacts in tactile images. A widely adopted calibration technique involves pressing a ball of known radius onto the sensor pad at various points. The tactile images captured during this process, combined with the known geometry of the ball, establish the correspondence between RGB changes and the local surface gradient at different locations. This method generates a sensor-specific look-up table. While traditional techniques assume pixel-invariant projection for simplicity,  neural networks can further learn precise and pixel-dependent projections. 
              <br><br>
              In the pre-training stage of SITR we adopt these steps to inform the model of sensor characteristics. We include a cube in our calibration to inform SITR about how the gel deforms around edges and corners. Thus, we press two objects—a 4mm diameter ball and a cube corner—at nine locations each, roughly arranged in a $3\times 3$ grid pattern across the sensor surface as seen in the figure above. These calibration images guide the encoder to identify and factor out sensor-specific features.
            </p>
          </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 is-size-4">Supervised Contrastive Learning</h2>
        <img src="static/images/tsne.png" alt="architecture" class="image is-fullwidth"/>
        <div class="content has-text-justified">
          <br><br>
          <p>
            To amplify the effect of the calibration tokens, we employ supervised contrastive learning to SITR. We label positive pairs from tactile images with the same contact geometry across multiple sensors, while negative pairs are labeled from images of different contact geometries or locations. In our batched implementation, we include two views for each sample: tactile images of the same contact captured by two different sensors. This approach allows us to learn discriminative features for downstream tasks while being robust to variations in sensor characteristics. 
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 is-size-4">Synthetic Dataset Generation</h2>
          <div class="is-flex is-justify-content-center">
            <img src="static/images/blender.png" alt="blender" class="image" style="max-width:60%;"/>
          </div>
          <div class="content has-text-justified">
            <br><br>
            <p>
              To train this model, we construct a large-scale synthetic dataset that spans a wide range of tactile sensor configurations, providing tactile signals of contact geometries along with their corresponding normal maps. 
              Physically-Based Rendering (PBR) simulates the camera images by tracing the path of light rays traveling in the scene and how they interact with optical components. 
              Therefore, the technology models the physical behavior of the optical system and can simulate a GelSight sensor's reading with parameterized optical settings. 
              To mimic the variance across real-world tactile sensors, we identify key parameters that highlight the differences between real-world tactile sensors.
              This includes light properties (shape, orientation, angle, color), gel properties (stiffness, specularity), and camera properties (FOV, sensing area).
            </p>
          </div>
      </div>
    </div>
  </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <br>
        <img src="static/images/reconstruction.png" alt="reconstruction" class="image is-fullwidth"/>
        <div class="content has-text-justified">
          <br><br>
          <p>
            Reconstruction examples for various sensors. The top row shows input tactile images, the middle row presents 3D reconstructions, and the bottom row shows the contact objects. Simulated sensors (Simulation 1 and 2) are in the training set, while real sensors (GelSight Mini, DIGIT, Hex, Wedge) are not.
            We qualitatively show how SITR preserves geometry and texture information across sensors by reconstructing the contact height map.    
          </p>

          <div class="content has-text-justified">
            <p>
              We conduct classification and pose estimation experiments with multiple tactile sensors that can be divided into two groups:
            </p>
            <ul>
              <li>
                <strong>Intra-sensor set:</strong> GelSight Mini 1 to 4 of different gel pads. These sensors have the same optical design, i.e., placement of camera and light sources, but differ in brightness and color of tactile signals due to manufacturing differences and choice of coating materials.
              </li>
              <li>
                <strong>Inter-sensor set:</strong> GelSight Mini 1, GelSight Wedge, GelSight Hex, and DIGIT. These sensors are designed with very different optical structures and, therefore, generate tactile signals that are significantly different from each other.
              </li>
            </ul>
          </div>

          <img src="static/images/tab1.png" alt="table1" class="image is-fullwidth"/>
          <p>
            <strong>Table 1:</strong> Results of object classification accuracy on 16 classes for model transfer and no-transfer performance.  
            We report the mean and standard deviation of transfer accuracy percent among the sensor sets specified.
            Random guess classification accuracy corresponds to $6.67\%$.
          </p>

          <img src="static/images/tab2.png" alt="table2" class="image is-fullwidth"/>
          <p>
            <strong>Table 2:</strong> Results of pose estimation with 6 objects. We report the mean and standard deviation of transfer pose estimation root mean square error (RMSE) in $mm$ among the sensor sets specified. 
            Random guess pose estimation RMSE corresponds to $2.52 mm$.
          </p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
<pre><code>@misc{gupta2025sensorinvarianttactilerepresentation,
    title={Sensor-Invariant Tactile Representation}, 
    author={Harsh Gupta and Yuchen Mo and Shengmiao Jin and Wenzhen Yuan},
    year={2025},
    eprint={2502.19638},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    url={https://arxiv.org/abs/2502.19638}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">template</a>
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
